{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afce2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR/Jupyter_Notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-16:] == 'image-to-tex-OCR':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07ea162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Data_Module import Data_Module\n",
    "from Models.Printed_Tex_Transformer import ResNetTransformer\n",
    "from Lightning_Models.Printed_Tex_Lit_Model import LitResNetTransformer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, Math\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb68b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Module by uploading images and formulas\n",
    "# images need to be in the folder Data/Data_Bank/generated_png_images\n",
    "# formulas need to be in Data/Data_Bank/final_png_formulas.txt\n",
    "# image filenames need to be in Data/Data_Bank/corresponding_png_images.txt\n",
    "\n",
    "dataset = Data_Module(stage = 'fit',\n",
    "                 set_max_label_length = 128,\n",
    "                 number_png_images_to_use_in_dataset=250*1000,\n",
    "                 labels_transform='default',\n",
    "                image_transform_name ='alb',\n",
    "                 train_val_fraction = 0.999,\n",
    "                   vocabulary_path = 'Data/Data_Bank/230k.json',\n",
    "                load_vocabulary = True,\n",
    "                      image_padding = True,\n",
    "\n",
    "\n",
    "                 batch_size = 128,\n",
    "                num_workers = 8,\n",
    "                data_on_gpu = True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb165d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 579 \n",
      "Max label length: 130 \n",
      "Start <S> goes to index  0 \n",
      "End <E> goes to index  1 \n",
      "Padding <P> goes to index  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmin_h: 9\\nmax_h: 177\\nmin_w: 24\\nmax_w: 512\\nmin_ratio: 0.42857142857142855\\nmax_ratio: 35.357142857142854\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \n",
    "    'Vocabulary size:',len(dataset.vocabulary),\n",
    "    '\\nMax label length:', dataset.max_label_length,\n",
    "    \"\\nStart <S> goes to index \",dataset.vocabulary['<S>'],\n",
    "      \"\\nEnd <E> goes to index \",dataset.vocabulary['<E>'],\n",
    "      \"\\nPadding <P> goes to index \",dataset.vocabulary['<P>'],\n",
    ")\n",
    "'''\n",
    "min_h: 9\n",
    "max_h: 177\n",
    "min_w: 24\n",
    "max_w: 512\n",
    "min_ratio: 0.42857142857142855\n",
    "max_ratio: 35.357142857142854\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNetTransformer(dataset=dataset).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4390ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load((\"Models_Parameters_Log/Printed3_nocompression.pth\"), map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab200a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LitModel  = LitResNetTransformer(model=model)\n",
    "#model = LitModel.load_from_checkpoint(\"Models_Parameters_Log/Printed2_inverted.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427cc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "transform = transforms.ToPILImage()\n",
    "# Helper Function to convert prediction labels to strings\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35ccebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  LongestMaxSize(always_apply=True, p=1, max_size=420, interpolation=2),\n",
       "  PadIfNeeded(always_apply=True, p=1.0, min_height=420, min_width=420, pad_height_divisor=None, pad_width_divisor=None, border_mode=0, value=0, mask_value=None),\n",
       "  CenterCrop(always_apply=True, p=1.0, height=250, width=420),\n",
       "  ToGray(always_apply=True, p=0.5),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.image_transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449d003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89dc9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 424, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> n : = \\mathrm { c l e g } ( f ) = \\, q ^ { 3 } + { \\bf 1 } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> n : = \\mathrm { c l e g } ( f ) = \\, q ^ { 3 } + { \\bf 1 } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/my_image.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "\n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    \n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5acbc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 516, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD6CAAAAADL1GzaAAAFRklEQVR4nO3ZW4hVVRzH8d/aZ0THGa+TimgZE+GgWJHkmxThg5YVXYSoJpAoyiApoQcllRCiKR+UDEIQSejClCnZQ/mSTUFmRkZiCVqYjag44y3HbM759bD3PmfPTec4Xg76/TzMrPtee/1n77XOGQkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgJI/ehbt/uzKT+OaFg2od5AbcsXc842NY6RHNNgbyhhhTGPjY1I0aUATuca5ZNOY8nvXhSQ1aJL/OnTIXzVYQf6wvwNUx5eeJJd97evIrTeesMfV19dPcMFDy3uwonmH0xjJfjL+ZUnf9XPJo9HOt9TX1zjuhj61xQsUtNmeUVbPJzwvSUXrPF+SotmeEEn1/jfbLgrvrlnzZvHV2lyqsZslRZpPkM6vPV2gZ+znwvnbdrXxeDHpj+Oej/t2SWo9NjjTbqR98gd7SDJ4KR5NnX4wSFL4uGKDNLBd+3J4r6y1eviTUro66dm+S5JeGz0x025hYejwGTWFjqTJnmLF8Zw2W5KcU6WqvCAtiP/YmySp+FCFeJ6ZdYyS+rq0wIU5ceKrWZKkWn+dGXV5rkM6k9sSBymfL1aMyMuntkrSY/dcmhu4VqWvu9wjzi+RcpPfse1N8cFvxFG7Rra36w3bO+LSFZKk7D7igtevzAQ2W7dvR5LY5E+XL/WuzLWTc+WZZXWV+yhVhCRI0yfarZKGu1PSL4VVQa/avsO2tN32jdKRzh2+QcPyrpWkjq5Bct6eVsxn6tYtSFMtLfE1UtHY4vm/qqzd8LrTbnecPdthW0HS1sIBSUNsxQfquz1Pkj1YSte+3R9JXYOUPhOl7Iu9XmzI4K7B+KcYpUt4R5dU1dWeQGr2UEnVG4MlzdJNks4mNfVhW7Kq8aF6Z5zpsaQhaO6R7fLmh9KS3t9fZ7vla6aOapHyOfnk8AHcwGVUGUGytK2YktSZjUBtt3C0ltqVRAVZ2hIF60wvF3g21/WEVPNWJrM7CtKGpxxce7r8uV8BlRGk7ptBujtEhf72KDhIUiGEXl9aa08f7pLfVkytWFLXVpD0dKM1fBxBupwW7ot/e4+GpWWuLtXX9tXxjNqCJVmfP6Cceo/xVVZ5n5Mk7VV8wG5b3d8euWOdSWqs5qaFYWe/+iYHhtChv/fm6hYe2v/FS41JqL7p8cRer6Lid3epRT4sRZpmBdk3J6W2FCRvlILa/H5allSOkCS97tm5TPsLW+zC2umSNNdeLVlfHlG65f15zkMHcGPXFNsu2MuLBaHR3qLxXhSqbLtpmKSptv1ofMy+T7bdKUn+IBmj3a+MGjWqqfO/dJAXCpkgtbibYs1it59LypqCtENepTRItqdcvtvuv0rZk4I0qJjxhp2777caflfDlCAdPVIt/Tq5Stod1JA7dWDi6ClBJw7e+ZOUnOWav5w9vnWlpNp/0kHOZd9UM2/rVNapYqqmffT46KAk3bJfCndJL0vrk0l5QqsgnWdbDJkGSaOqUnH81XXzvq7DlAb7/mS/N9yQ+fm2Je1rTgZjS7oUlpa+Buqm//+a7drtoCQv05yLnhJ6+LmPF9KWizxL+15JVp8f0FC+kOv1H+5B/vHiBpw5Ugo3z71gO5Qhksf0iFIU/O3VmAz61MuLLf/blZ8GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcJH+B1YnFXQEyIL7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\mathrm { P e r m } ( S ) \\cong { \\bf S } _ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\mathrm { P e r m } ( S ) \\cong { \\bf S } _ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/zarhin.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    pass\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7f98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 658, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c044477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "#Thresholding\n",
    "ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06c580c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 178, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> h ( a , b , c ; \\eta ) = \\sum _ { n = 0 } ^ { \\infty } \\frac { ( a ) _ { n } ( b ) _ { n } } { n ! ( c ) _ { n } } \\eta ^ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> h ( a , b , c ; \\eta ) = \\sum _ { n = 0 } ^ { \\infty } \\frac { ( a ) _ { n } ( b ) _ { n } } { n ! ( c ) _ { n } } \\eta ^ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/0a0df4c4229b725.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc374a31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 57, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD6CAAAAADL1GzaAAAFk0lEQVR4nO3ay29UZRjH8eecM5d2pnNpS69AW+iFtkDBFBAIRQ1CouLKqBt37vxT3Ll35dKQmKhxoQSMGiUWUGipgNA2tNDLtHPpzLSd2zkueplpizBNOsy8ne9nMznvnJ48yW/ea48IAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKCMaMUuYJv0YheAl1PtR7UzPJ16sLd+cKjYdeTJVuwCXgXdsExTdMtaa/CesobPv6Y/iWWKWVbedv9wp4nUdrdWieZxrrfZvNr00OOaHlcR69qG3d+TLMPf2aqP35WG0PLKb9KyFh/NPmmPJLJdq7Tt/pCkqW+P70DjfHxPMmjYxBLNWg5dD1st7bduqTHalcPCoanL0Jt9iYnxBb3SYYkYycC8SNtp3407xS4tT2XQk+Yi6UzmzCfOq/Yej9O0xLEYFekeGP/RtW82Wezi8lIGIaVSIjIVXk6lpkOGJWIkZz19bx+eaw2MBgmplGSGA0bm6dpV3ZE2vcv/+0MWDiVE9/rmAjmJxP6asVUujj1ToyOVw8JBRBwd1aGZYDYl3WFYurlsFrGk7SiPnqRVGbPzOddmUkRTZZdULiFlAlp4Q4MqfaisaOUxrBeftuEjr3tfejapxtGlUr8wtzOVSOU5k+hOpy2zmBCj0mXXnv83xnIotYPVFY5Sc9LZQxMjY+mX36dZIs6+/saZn+9J7Zk3Oiqeu9TWam98MbbTJRaESiHVvjtwLfAkj5AskbrTh0zf/oWpYB63lzyFQtIPHu++nedgV9n7cfzzug/aT92cu/rHC4a7HSyvgFQJSbOk670ue5Urv0n0wmXHb4+XIi3Hx+fi8RfcpyuxFldjeSNiiXQcMyd1T14Fuy9fGvxavAl3vWNDu6br2obFkhIZKdOTxNNeNVJTU+HOpye5+9qth0GxV3kXjNx2x7nDmT9vOvfNRgtUZIEoE1LLhac/uAZcnmxImrZhi2pZ6+c8HRd9j+dFGltcSxvWDfUHj2qRR/6GKCEVRk3v1L3X05U5c5LhdBjZFYGWSSYyq5fN/bbJSqezpdUezd0ItbXeH/XJiUxEjd1RliIh2fc2jw6Fp+J+73pImrfBX5l9ScFIhGbCq5ct/dFo64B24tCDsVjOQ/T4+FzVwLkHI7mNKlAkJP87ju+HxEw4sz1J8+xv8mT7hD0+tRxdDam6bklr9ho9jYOjuQPbk6dpiZl7H868qqp3iiIhtbzvjtU0nN9XW7EekhUZCzjN9fHOSMbWOlJVs+P+dwlPnaFPD0dyHpJOi0gooMjmKIcSIemeLm/92R5fZ011Tkjh8PPvttd607d/Eu/J+dnR4JZvF+5MF6rMgin9kDRLzEv9VyZDseqjF9+ozrbXNvoqcuakZGR65QzI5pZwSKTiWMXgv5sfZvO6w6rNSCqEZIk0HnNcmRSR8YYBf3Yz62pqzJ2TFqeiKwOZmYjFEiId5zPfbHmxzl4j05HNjSWv5EPSTak+PvdgSUQkGtXt2Y1PZGy+IndOioRWzg8SgbgzI9LSOXRt6yG3GZp/0TFRaSr5kMzq3ouHU786RVwHjx5xtH10PRA1RUSsWHJ2wz4plVw95FmYCDR2H+i+fXVMNh+tpiNJxTayIgqEJN2fveVKxq4/k/1vnjuZ2fOp/9vlhIiIZFY/12RfLBm91/Bh7/SXt0Q2H3+n1VvaiQohBX6ZMJPDQZGFf5b+1vXUSGTtVPR/X/e5+5VDn5kcXNz6jTpvCClF2/RZjlT5V8V2QirnPAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgl/kPgAS5/kHxWQYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> A = \\theta ^ { \\frac { \\gamma - 2 } { \\gamma } } \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> A = \\theta ^ { \\frac { \\gamma - 2 } { \\gamma } } \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/0a0ebebfb1f6ab3.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xs(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a8d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1080, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\rho \\left( \\frac { \\partial \\, V } { \\partial t } + \\, V \\cdot \\nabla \\nu \\right) = - \\nabla p + \\nabla \\cdot T + f <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\rho \\left( \\frac { \\partial \\, V } { \\partial t } + \\, V \\cdot \\nabla \\nu \\right) = - \\nabla p + \\nabla \\cdot T + f <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_3.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d4c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1424, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=470x280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> X \\left( t _ { n } \\right) = i _ { m - 1 } ^ { s } | X ; t _ { 0 } | \\left( t _ { n } \\right) + \\frac { 1 } { \\Gamma ( \\beta ) } \\sum _ { f = 0 } ^ { n - 1 } \\int _ { f _ { r } } ^ { f _ { r } . . . t } ( t _ { n } - r ) ^ { f - 1 } \\kappa \\left( r , X ( \\tau ) \\right) d \\tau . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> X \\left( t _ { n } \\right) = i _ { m - 1 } ^ { s } | X ; t _ { 0 } | \\left( t _ { n } \\right) + \\frac { 1 } { \\Gamma ( \\beta ) } \\sum _ { f = 0 } ^ { n - 1 } \\int _ { f _ { r } } ^ { f _ { r } . . . t } ( t _ { n } - r ) ^ { f - 1 } \\kappa \\left( r , X ( \\tau ) \\right) d \\tau . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_14.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xl(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d1ebb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 1774, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\chi ( t _ { 2 } ) = \\Gamma _ { m - 1 } | \\chi _ { 1 } | \\{ t _ { q } \\rangle * \\frac { 1 } { 1 / \\delta \\beta } \\sum _ { r = 0 } ^ { - 1 } \\int _ { f _ { r } } ^ { 4 - 1 } ( t _ { n } - \\Gamma ) \\beta ^ { - 1 } { \\cal F } \\, [ \\gamma \\, \\chi ( \\tau ) ) d \\tau . \\qquad \\qquad \\qquad ( h ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\chi ( t _ { 2 } ) = \\Gamma _ { m - 1 } | \\chi _ { 1 } | \\{ t _ { q } \\rangle * \\frac { 1 } { 1 / \\delta \\beta } \\sum _ { r = 0 } ^ { - 1 } \\int _ { f _ { r } } ^ { 4 - 1 } ( t _ { n } - \\Gamma ) \\beta ^ { - 1 } { \\cal F } \\, [ \\gamma \\, \\chi ( \\tau ) ) d \\tau . \\qquad \\qquad \\qquad ( h ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2fece0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 1950, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=470x280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> V _ { T } = V _ { 1 } ^ { 2 } z \\cdot Y _ { 2 U } ^ { ( U ) } = \\sum _ { j = 1 } ^ { N } A _ { T } \\rightarrow j _ { T } ^ { 2 } } A _ { T } ( X _ { f } ) \\times \\sum _ { j = - U ^ { \\prime } } ^ { N } d _ { T } / \\{ X _ { f } \\} \\times \\sum _ { f = 0 } ^ { D } d _ { q - j } ( X _ { f } ) , \\quad q =\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> V _ { T } = V _ { 1 } ^ { 2 } z \\cdot Y _ { 2 U } ^ { ( U ) } = \\sum _ { j = 1 } ^ { N } A _ { T } \\rightarrow j _ { T } ^ { 2 } } A _ { T } ( X _ { f } ) \\times \\sum _ { j = - U ^ { \\prime } } ^ { N } d _ { T } / \\{ X _ { f } \\} \\times \\sum _ { f = 0 } ^ { D } d _ { q - j } ( X _ { f } ) , \\quad q =$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_10.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xl(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc905c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a97c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e95d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1053058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b53b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8e91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbb5059",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data_Module' object has no attribute 'data_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DISREGARD this uses wrong transformations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get image and label from train data -- change number for different ones\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m image_tensor, label \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_train\u001b[49m[\u001b[38;5;241m12\u001b[39m]\n\u001b[1;32m      5\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToPILImage()\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image_tensor)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data_Module' object has no attribute 'data_train'"
     ]
    }
   ],
   "source": [
    "# DISREGARD this uses wrong transformations\n",
    "\n",
    "# Get image and label from train data -- change number for different ones\n",
    "image_tensor, label = dataset.data_train[12]\n",
    "transform = transforms.ToPILImage()\n",
    "image = transform(image_tensor)\n",
    "\n",
    "print('\\nOriginal image and formula:')\n",
    "display(image)\n",
    "print(token_to_strings(label))\n",
    "print('\\nPredicted formula: \\n')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prediction =  model.predict(image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(prediction),'\\n')\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# My first app\n",
    "Here's our first attempt at using data to create a table:\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "  'first column': [1, 2, 3, 4],\n",
    "  'second column': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    np.random.randn(10, 20),\n",
    "    columns=('col %d' % i for i in range(20)))\n",
    "\n",
    "st.dataframe(dataframe.style.highlight_max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dba747",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = pd.DataFrame(\n",
    "    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n",
    "    columns=['lat', 'lon'])\n",
    "\n",
    "st.map(map_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db861789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "x = st.slider('x')  #  this is a widget\n",
    "st.write(x, 'squared is', x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(float(.6)*420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c218787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
