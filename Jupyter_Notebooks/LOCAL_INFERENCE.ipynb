{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afce2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR/Jupyter_Notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-16:] == 'image-to-tex-OCR':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07ea162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Data_Module import Data_Module\n",
    "from Models.Printed_Tex_Transformer import ResNetTransformer\n",
    "from Lightning_Models.Printed_Tex_Lit_Model import LitResNetTransformer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, Math\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb68b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Module by uploading images and formulas\n",
    "# images need to be in the folder Data/Data_Bank/generated_png_images\n",
    "# formulas need to be in Data/Data_Bank/final_png_formulas.txt\n",
    "# image filenames need to be in Data/Data_Bank/corresponding_png_images.txt\n",
    "\n",
    "dataset = Data_Module(stage = 'fit',\n",
    "                 set_max_label_length = 128,\n",
    "                 number_png_images_to_use_in_dataset=250*1000,\n",
    "                 labels_transform='default',\n",
    "                image_transform_name ='alb',\n",
    "                 train_val_fraction = 0.999,\n",
    "                   vocabulary_path = 'Data/Data_Bank/230k.json',\n",
    "                load_vocabulary = True,\n",
    "                      image_padding = True,\n",
    "\n",
    "\n",
    "                 batch_size = 128,\n",
    "                num_workers = 8,\n",
    "                data_on_gpu = True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb165d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 579 \n",
      "Max label length: 130 \n",
      "Start <S> goes to index  0 \n",
      "End <E> goes to index  1 \n",
      "Padding <P> goes to index  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmin_h: 9\\nmax_h: 177\\nmin_w: 24\\nmax_w: 512\\nmin_ratio: 0.42857142857142855\\nmax_ratio: 35.357142857142854\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \n",
    "    'Vocabulary size:',len(dataset.vocabulary),\n",
    "    '\\nMax label length:', dataset.max_label_length,\n",
    "    \"\\nStart <S> goes to index \",dataset.vocabulary['<S>'],\n",
    "      \"\\nEnd <E> goes to index \",dataset.vocabulary['<E>'],\n",
    "      \"\\nPadding <P> goes to index \",dataset.vocabulary['<P>'],\n",
    ")\n",
    "'''\n",
    "min_h: 9\n",
    "max_h: 177\n",
    "min_w: 24\n",
    "max_w: 512\n",
    "min_ratio: 0.42857142857142855\n",
    "max_ratio: 35.357142857142854\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNetTransformer(dataset=dataset).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4390ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load((\"Models_Parameters_Log/Printed4_nocompression.pth\"), map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab200a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LitModel  = LitResNetTransformer(model=model)\n",
    "#model = LitModel.load_from_checkpoint(\"Models_Parameters_Log/Printed2_inverted.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427cc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "transform = transforms.ToPILImage()\n",
    "# Helper Function to convert prediction labels to strings\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35ccebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "  LongestMaxSize(always_apply=True, p=1, max_size=420, interpolation=2),\n",
       "  PadIfNeeded(always_apply=True, p=1.0, min_height=420, min_width=420, pad_height_divisor=None, pad_width_divisor=None, border_mode=0, value=0, mask_value=None),\n",
       "  CenterCrop(always_apply=True, p=1.0, height=250, width=420),\n",
       "  ToGray(always_apply=True, p=0.5),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.image_transform_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab1d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f89dc9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 424, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD6CAAAAADL1GzaAAAGGUlEQVR4nO3aaWwUZRzH8d9Mi+DVClQhJh54FCESY3jjiaAmjRrjfaAmokgkDSqKkcQj6gsFTcQzRmMixisa0JCghheKeEGtR2IgEjEqIEo5aks4pLTdny/2mNnubndbSbcs38+LduaZZ57n3/nPs88zs5UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMABICx3AIUE5Q5g8Ai8fatGnrQ7cLkj6WnQ3j0D75C2o9p27t311aDLESLHeKJ0ZMuqcseRq7rcAQwe24JA2rm73GGgqB88tdwhoFendm/zYJyRKmThkGeR2o91679frvpF9f8/GuSo0rrVOYVn/tW/+6/6T5/7vyPa3w6AhUO91vV6vNvaE+2N7ZKr12nIse7XI2DXXo0qodrR2/rTeAUrNk1c6N+jUbPAth+QNNEbh/Wtn/OPCCX97Kt7rxZK0u6FfWu74hVJ0mlORDvP+C0FQeq0mr71c70l3WlfU6Te94lEYlCuL8qpyAWZ51fSm2GtnV4wVCnaLs3EBbbt6dmluTNbh33QJanohSxyQeKH23x6Zjt81c/2MZRx9fX143oWrtg4ObvgpPoaH0xJChf71yEFj6bSl3tBguhoIK+PSu3oNN3W9UHJgfRyp2zwZTllA5ykgVzd3Xv+8Clfb1XdpKruVEnNNTql4aNC9f3aSKluUnInrH1vt7TiBUnyl9v19odfbw3f/NCP66F09S82SUu6JoxN7S9cUNIrnhmXSM3zP626ZG+//qhK844T3rfNiegurLHflKSRwzNGRPWftt36r7uT9e3W7fYVkma4s7XN7mhtny01e0a6vm3bJ2dGRbvPixqLuhiRNc1cnDwr4SMKRD0IRtJAesd+7ga97MSnmaIznlYgqdOR9KEqubvtKZ37sC1pvL1CusuWtNzXSd/6Y0lS8z+ZWWTeH1726PzoMafdZ0edx3oYG5XqBvuhRx973246pEDUB1mS3vBLkkbZS3seGZ5vJO11eghJusOrJOkWK0gWzHEy1c1ro2Y+9z3xVnfEk5R/JE21Z0nSZ55bKOo/PEVS9rRVuXPSWbdKkgLp8p5ffrblqz9UQWbyqn7NtQ2HSePVMVSpZlIpjE6oHaVD4w20xB+UWvPGdJHWvCSp5uh8L5HuekGSlFiejrtcBjBJ+9QxV6pqUdaVlaSsnGUSI41uiYo9bpkkqSu1vyP1+iZ27S4Yt+X5eLOjY9thQvlMV4MknTUh3+H3frDU/vmo+1dKKvRpWGGu9R5JSi8EJEm1p0nKmjCig3ZdkPwtVUfloWbZttuS+Wn+PXPCZd6Q1WGhOenUeKkkabL/Khj2IJiTBnAkLepKfhiFczJFw1Yfd/0iSfdFi+X4Y9OrmTc03Usvv/315GZCL85+TlozITU4xtyd/FRSuCWny6HRZtTFsJasOmFCur+/X9kUGKAHrsw6IFrs1tqeXKj+4alHU1vSLPtKSQoCKfZaQWqKL8HXZzXQvriEmH5MdvF3wSq9j6RkVJUidbF1XjxJwzZ4z4mFzhi6wX5lZpP3WZLm2NMaGxfaqU+uz2beU6tQobwkfYKdNSDmJ4q/cVhrv9+4zPtceCjlJGla4022pzbeKMkdlbUWX5lM0nfeGXtsrDm5t29vErZ9SuqunWbbvuX4aHrZ3CTFU9Njopjr5cWj2m7bo3ubYnb5qjxR2d4i6bDKemAKJQV9W8iGOuE3SVJVqmCmJMn+6Unp0m9We7MkrfSWTA+x4RDUlPIWPNQxU6TOzsKXOmeEhfGtkH8vVd5L0Plxsvgcb5YUjLGvzXfeN7651E42eV7/I0Q+3Z5UVzeiru7q1BB6wrvG5NZalSj9S79Nbthv0UGS9OAntnfYfnd2suARv5FTqcE+sdQGz9zpK/ZPaMgYklw6ZKYqzc2dUi7qw4T+oJv2S1yIq4r9VM52Sp8eT5n9AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOAA9x9vMIF33MAX7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> n : = \\mathrm { d e g } ( f \\, ) \\, = \\, q ^ { 3 } + { \\bf 1 } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> n : = \\mathrm { d e g } ( f \\, ) \\, = \\, q ^ { 3 } + { \\bf 1 } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/my_image.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "\n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    \n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5acbc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 516, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD6CAAAAADL1GzaAAAFRklEQVR4nO3ZW4hVVRzH8d/aZ0THGa+TimgZE+GgWJHkmxThg5YVXYSoJpAoyiApoQcllRCiKR+UDEIQSejClCnZQ/mSTUFmRkZiCVqYjag44y3HbM759bD3PmfPTec4Xg76/TzMrPtee/1n77XOGQkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgJI/ehbt/uzKT+OaFg2od5AbcsXc842NY6RHNNgbyhhhTGPjY1I0aUATuca5ZNOY8nvXhSQ1aJL/OnTIXzVYQf6wvwNUx5eeJJd97evIrTeesMfV19dPcMFDy3uwonmH0xjJfjL+ZUnf9XPJo9HOt9TX1zjuhj61xQsUtNmeUVbPJzwvSUXrPF+SotmeEEn1/jfbLgrvrlnzZvHV2lyqsZslRZpPkM6vPV2gZ+znwvnbdrXxeDHpj+Oej/t2SWo9NjjTbqR98gd7SDJ4KR5NnX4wSFL4uGKDNLBd+3J4r6y1eviTUro66dm+S5JeGz0x025hYejwGTWFjqTJnmLF8Zw2W5KcU6WqvCAtiP/YmySp+FCFeJ6ZdYyS+rq0wIU5ceKrWZKkWn+dGXV5rkM6k9sSBymfL1aMyMuntkrSY/dcmhu4VqWvu9wjzi+RcpPfse1N8cFvxFG7Rra36w3bO+LSFZKk7D7igtevzAQ2W7dvR5LY5E+XL/WuzLWTc+WZZXWV+yhVhCRI0yfarZKGu1PSL4VVQa/avsO2tN32jdKRzh2+QcPyrpWkjq5Bct6eVsxn6tYtSFMtLfE1UtHY4vm/qqzd8LrTbnecPdthW0HS1sIBSUNsxQfquz1Pkj1YSte+3R9JXYOUPhOl7Iu9XmzI4K7B+KcYpUt4R5dU1dWeQGr2UEnVG4MlzdJNks4mNfVhW7Kq8aF6Z5zpsaQhaO6R7fLmh9KS3t9fZ7vla6aOapHyOfnk8AHcwGVUGUGytK2YktSZjUBtt3C0ltqVRAVZ2hIF60wvF3g21/WEVPNWJrM7CtKGpxxce7r8uV8BlRGk7ptBujtEhf72KDhIUiGEXl9aa08f7pLfVkytWFLXVpD0dKM1fBxBupwW7ot/e4+GpWWuLtXX9tXxjNqCJVmfP6Cceo/xVVZ5n5Mk7VV8wG5b3d8euWOdSWqs5qaFYWe/+iYHhtChv/fm6hYe2v/FS41JqL7p8cRer6Lid3epRT4sRZpmBdk3J6W2FCRvlILa/H5allSOkCS97tm5TPsLW+zC2umSNNdeLVlfHlG65f15zkMHcGPXFNsu2MuLBaHR3qLxXhSqbLtpmKSptv1ofMy+T7bdKUn+IBmj3a+MGjWqqfO/dJAXCpkgtbibYs1it59LypqCtENepTRItqdcvtvuv0rZk4I0qJjxhp2777caflfDlCAdPVIt/Tq5Stod1JA7dWDi6ClBJw7e+ZOUnOWav5w9vnWlpNp/0kHOZd9UM2/rVNapYqqmffT46KAk3bJfCndJL0vrk0l5QqsgnWdbDJkGSaOqUnH81XXzvq7DlAb7/mS/N9yQ+fm2Je1rTgZjS7oUlpa+Buqm//+a7drtoCQv05yLnhJ6+LmPF9KWizxL+15JVp8f0FC+kOv1H+5B/vHiBpw5Ugo3z71gO5Qhksf0iFIU/O3VmAz61MuLLf/blZ8GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcJH+B1YnFXQEyIL7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\mathrm { P e r m } ( S ) \\cong { \\bf S } _ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\mathrm { P e r m } ( S ) \\cong { \\bf S } _ { n } . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/zarhin.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    pass\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef7f98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 658, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\quad p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\quad p ( X = x _ { i } ) = \\sum _ { j = 1 } ^ { L } p ( X = x _ { i } , Y = y _ { j } ) \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "print(my_image.shape)\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2211ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23100303951367782\n",
      "(152, 658, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> p ( X \\, = \\, x _ { i } ) \\, = \\sum _ { j = 1 } ^ { L } p ( X \\, = x _ { i } , \\, Y \\, \\equiv y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> p ( X \\, = \\, x _ { i } ) \\, = \\sum _ { j = 1 } ^ { L } p ( X \\, = x _ { i } , \\, Y \\, \\equiv y _ { j } ) <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "#Thresholding\n",
    "ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "h,w,c = my_image.shape\n",
    "aspect = h/w\n",
    "print(h/w)\n",
    "\n",
    "if w>1200:\n",
    "    new_w = 1000\n",
    "    new_h = int(new_w*aspect)\n",
    "    my_image= cv2.resize(my_image, (new_w, new_h), interpolation= cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c580c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2303370786516854\n",
      "(41, 178, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> h ( a , b , c ; \\eta ) = \\sum _ { n = 0 } ^ { \\infty } \\frac { ( a ) _ { n } ( b ) _ { n } } { n ! ( c ) _ { n } } \\eta ^ { n } , <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> h ( a , b , c ; \\eta ) = \\sum _ { n = 0 } ^ { \\infty } \\frac { ( a ) _ { n } ( b ) _ { n } } { n ! ( c ) _ { n } } \\eta ^ { n } , <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/0a0df4c4229b725.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "aspect = h/w\n",
    "print(h/w)\n",
    "\n",
    "if w>1200:\n",
    "    new_w = 1000\n",
    "    new_h = int(new_w*aspect)\n",
    "    my_image= cv2.resize(my_image, (new_w, new_h), interpolation= cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_small(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc374a31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 57, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAD6CAAAAADL1GzaAAAFk0lEQVR4nO3ay29UZRjH8eecM5d2pnNpS69AW+iFtkDBFBAIRQ1CouLKqBt37vxT3Ll35dKQmKhxoQSMGiUWUGipgNA2tNDLtHPpzLSd2zkueplpizBNOsy8ne9nMznvnJ48yW/ea48IAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKCMaMUuYJv0YheAl1PtR7UzPJ16sLd+cKjYdeTJVuwCXgXdsExTdMtaa/CesobPv6Y/iWWKWVbedv9wp4nUdrdWieZxrrfZvNr00OOaHlcR69qG3d+TLMPf2aqP35WG0PLKb9KyFh/NPmmPJLJdq7Tt/pCkqW+P70DjfHxPMmjYxBLNWg5dD1st7bduqTHalcPCoanL0Jt9iYnxBb3SYYkYycC8SNtp3407xS4tT2XQk+Yi6UzmzCfOq/Yej9O0xLEYFekeGP/RtW82Wezi8lIGIaVSIjIVXk6lpkOGJWIkZz19bx+eaw2MBgmplGSGA0bm6dpV3ZE2vcv/+0MWDiVE9/rmAjmJxP6asVUujj1ToyOVw8JBRBwd1aGZYDYl3WFYurlsFrGk7SiPnqRVGbPzOddmUkRTZZdULiFlAlp4Q4MqfaisaOUxrBeftuEjr3tfejapxtGlUr8wtzOVSOU5k+hOpy2zmBCj0mXXnv83xnIotYPVFY5Sc9LZQxMjY+mX36dZIs6+/saZn+9J7Zk3Oiqeu9TWam98MbbTJRaESiHVvjtwLfAkj5AskbrTh0zf/oWpYB63lzyFQtIPHu++nedgV9n7cfzzug/aT92cu/rHC4a7HSyvgFQJSbOk670ue5Urv0n0wmXHb4+XIi3Hx+fi8RfcpyuxFldjeSNiiXQcMyd1T14Fuy9fGvxavAl3vWNDu6br2obFkhIZKdOTxNNeNVJTU+HOpye5+9qth0GxV3kXjNx2x7nDmT9vOvfNRgtUZIEoE1LLhac/uAZcnmxImrZhi2pZ6+c8HRd9j+dFGltcSxvWDfUHj2qRR/6GKCEVRk3v1L3X05U5c5LhdBjZFYGWSSYyq5fN/bbJSqezpdUezd0ItbXeH/XJiUxEjd1RliIh2fc2jw6Fp+J+73pImrfBX5l9ScFIhGbCq5ct/dFo64B24tCDsVjOQ/T4+FzVwLkHI7mNKlAkJP87ju+HxEw4sz1J8+xv8mT7hD0+tRxdDam6bklr9ho9jYOjuQPbk6dpiZl7H868qqp3iiIhtbzvjtU0nN9XW7EekhUZCzjN9fHOSMbWOlJVs+P+dwlPnaFPD0dyHpJOi0gooMjmKIcSIemeLm/92R5fZ011Tkjh8PPvttd607d/Eu/J+dnR4JZvF+5MF6rMgin9kDRLzEv9VyZDseqjF9+ozrbXNvoqcuakZGR65QzI5pZwSKTiWMXgv5sfZvO6w6rNSCqEZIk0HnNcmRSR8YYBf3Yz62pqzJ2TFqeiKwOZmYjFEiId5zPfbHmxzl4j05HNjSWv5EPSTak+PvdgSUQkGtXt2Y1PZGy+IndOioRWzg8SgbgzI9LSOXRt6yG3GZp/0TFRaSr5kMzq3ouHU786RVwHjx5xtH10PRA1RUSsWHJ2wz4plVw95FmYCDR2H+i+fXVMNh+tpiNJxTayIgqEJN2fveVKxq4/k/1vnjuZ2fOp/9vlhIiIZFY/12RfLBm91/Bh7/SXt0Q2H3+n1VvaiQohBX6ZMJPDQZGFf5b+1vXUSGTtVPR/X/e5+5VDn5kcXNz6jTpvCClF2/RZjlT5V8V2QirnPAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgl/kPgAS5/kHxWQYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\qquad A = \\theta ^ { \\frac { \\gamma - 2 } { \\gamma } } \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\qquad A = \\theta ^ { \\frac { \\gamma - 2 } { \\gamma } } \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/0a0ebebfb1f6ab3.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xs(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05c6349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1080, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=420x250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> \\partial \\left( \\frac { \\partial \\, V } { \\partial \\, t } + V \\cdot \\nabla V \\right) = - \\nabla p + \\nabla \\cdot T + f \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> \\partial \\left( \\frac { \\partial \\, V } { \\partial \\, t } + V \\cdot \\nabla V \\right) = - \\nabla p + \\nabla \\cdot T + f \\qquad <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_3.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_medium(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362edd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1404494382022472\n",
      "(140, 1000, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=470x280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> X ( t _ { n } ) = T _ { m - 1 } [ X ; t _ { 0 } ] ( t _ { n } ) + \\frac { 1 } { \\Gamma ( \\beta ) } \\sum _ { r = 0 } ^ { n - 1 } \\int _ { t _ { r } } ^ { t _ { r . . . 1 } } ( t _ { n } - r ) ^ { g - 1 } F ( r , X ( r ) ) d \\tau . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> X ( t _ { n } ) = T _ { m - 1 } [ X ; t _ { 0 } ] ( t _ { n } ) + \\frac { 1 } { \\Gamma ( \\beta ) } \\sum _ { r = 0 } ^ { n - 1 } \\int _ { t _ { r } } ^ { t _ { r . . . 1 } } ( t _ { n } - r ) ^ { g - 1 } F ( r , X ( r ) ) d \\tau . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_14.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "\n",
    "\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "\n",
    "h,w,c = my_image.shape\n",
    "aspect = h/w\n",
    "print(h/w)\n",
    "\n",
    "if w>1200:\n",
    "    new_w = 1000\n",
    "    new_h = int(new_w*aspect)\n",
    "    my_image= cv2.resize(my_image, (new_w, new_h), interpolation= cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xl(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "093dc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 1774, 3)\n",
      "(136, 1200, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=470x280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> X ( t _ { \\alpha } ) = T _ { m - 1 } [ X ; t _ { 0 } ] ( t _ { n } ) + \\frac { 1 } { \\Gamma ( { \\cal B } ) } \\sum _ { r = 0 } ^ { r - 1 } \\int _ { \\ell _ { r } } ^ { r _ { - 1 } } ( t _ { R } - \\tau ) ^ { \\beta - 1 } P ( \\tau , \\chi ( \\tau ) ) d \\Upsilon . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> X ( t _ { \\alpha } ) = T _ { m - 1 } [ X ; t _ { 0 } ] ( t _ { n } ) + \\frac { 1 } { \\Gamma ( { \\cal B } ) } \\sum _ { r = 0 } ^ { r - 1 } \\int _ { \\ell _ { r } } ^ { r _ { - 1 } } ( t _ { R } - \\tau ) ^ { \\beta - 1 } P ( \\tau , \\chi ( \\tau ) ) d \\Upsilon . <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "    \n",
    "\n",
    "\n",
    "h,w,c = my_image.shape\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    \n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "aspect = h/w\n",
    "print(my_image.shape)\n",
    "\n",
    "if w>1600:\n",
    "    new_w = 1200\n",
    "    new_h = int(new_w*aspect)\n",
    "    my_image= cv2.resize(my_image, (new_w, new_h), interpolation= cv2.INTER_AREA)\n",
    "    \n",
    "print(my_image.shape)\n",
    "\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xl(image=np.array(my_image))['image']#[:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf11d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 1950, 3)\n",
      "(188, 1950, 3)\n",
      "(115, 1200, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=470x280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> Y _ { q } = Y _ { 1 } ^ { 2 T } + Y _ { 2 q } ^ { ( q } = \\sum _ { j = 1 } ^ { 2 p } d _ { q - j f } f ( X _ { j } ) \\times \\sum _ { j = r } ^ { 3 T } d _ { q - j } f ( X _ { j } ) \\times \\sum _ { j = p } ^ { q } d _ { q - j } f ( X _ { j } ) , \\quad q = 3 r + 1 , y r . <E> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> Y _ { q } = Y _ { 1 } ^ { 2 T } + Y _ { 2 q } ^ { ( q } = \\sum _ { j = 1 } ^ { 2 p } d _ { q - j f } f ( X _ { j } ) \\times \\sum _ { j = r } ^ { 3 T } d _ { q - j } f ( X _ { j } ) \\times \\sum _ { j = p } ^ { q } d _ { q - j } f ( X _ { j } ) , \\quad q = 3 r + 1 , y r . <E> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_10.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Thresholding\n",
    "#ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "non_zero = True\n",
    "\n",
    "# Find first non_zero pixel and crop to it\n",
    "if non_zero:\n",
    "    positions = np.nonzero(my_image)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "\n",
    "    my_image = cv2.rectangle(my_image, (left, top), (right, bottom), (0,0,0), 0)\n",
    "print(my_image.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if w>400:\n",
    "    #Thresholding\n",
    "    ret,my_image = cv2.threshold(my_image,127,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "h,w,c = my_image.shape\n",
    "aspect = h/w\n",
    "print(my_image.shape)\n",
    "\n",
    "if w>1600:\n",
    "    new_w = 1200\n",
    "    new_h = int(new_w*aspect)\n",
    "    my_image= cv2.resize(my_image, (new_w, new_h), interpolation= cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "print(my_image.shape)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test_xl(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "model.eval()\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed39e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c6bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c9d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64ee26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12c32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbb5059",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data_Module' object has no attribute 'data_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DISREGARD this uses wrong transformations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get image and label from train data -- change number for different ones\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m image_tensor, label \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_train\u001b[49m[\u001b[38;5;241m12\u001b[39m]\n\u001b[1;32m      5\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToPILImage()\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image_tensor)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data_Module' object has no attribute 'data_train'"
     ]
    }
   ],
   "source": [
    "# DISREGARD this uses wrong transformations\n",
    "\n",
    "# Get image and label from train data -- change number for different ones\n",
    "image_tensor, label = dataset.data_train[12]\n",
    "transform = transforms.ToPILImage()\n",
    "image = transform(image_tensor)\n",
    "\n",
    "print('\\nOriginal image and formula:')\n",
    "display(image)\n",
    "print(token_to_strings(label))\n",
    "print('\\nPredicted formula: \\n')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prediction =  model.predict(image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(prediction),'\\n')\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# My first app\n",
    "Here's our first attempt at using data to create a table:\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "  'first column': [1, 2, 3, 4],\n",
    "  'second column': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    np.random.randn(10, 20),\n",
    "    columns=('col %d' % i for i in range(20)))\n",
    "\n",
    "st.dataframe(dataframe.style.highlight_max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dba747",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = pd.DataFrame(\n",
    "    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n",
    "    columns=['lat', 'lon'])\n",
    "\n",
    "st.map(map_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db861789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "x = st.slider('x')  #  this is a widget\n",
    "st.write(x, 'squared is', x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(float(.6)*420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c6519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
