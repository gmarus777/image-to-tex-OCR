{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62e4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR/Jupyter_Notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-16:] == 'image-to-tex-OCR':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9908b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Data_Module import Data_Module\n",
    "from Models.Printed_Tex_Transformer import ResNetTransformer\n",
    "from Lightning_Models.Printed_Tex_Lit_Model import LitResNetTransformer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, Math\n",
    "from Data.image_transforms import Image_Transforms\n",
    "import cv2\n",
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "transform = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d4cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Module by uploading images and formulas\n",
    "# images need to be in the folder Data/Data_Bank/generated_png_images\n",
    "# formulas need to be in Data/Data_Bank/final_png_formulas.txt\n",
    "# image filenames need to be in Data/Data_Bank/corresponding_png_images.txt\n",
    "\n",
    "dataset = Data_Module(stage = 'fit',\n",
    "                 set_max_label_length = 128,\n",
    "                 number_png_images_to_use_in_dataset=250*1000,\n",
    "                 labels_transform='default',\n",
    "                image_transform_name ='alb',\n",
    "                 train_val_fraction = 0.999,\n",
    "                   vocabulary_path = 'Data/Data_Bank/230k.json',\n",
    "                load_vocabulary = True,\n",
    "                      image_padding = True,\n",
    "                      max_width = 700,\n",
    "\n",
    "\n",
    "                 batch_size = 128,\n",
    "                num_workers = 8,\n",
    "                data_on_gpu = True,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8803846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 579 \n",
      "Max label length: 130 \n",
      "Start <S> goes to index  0 \n",
      "End <E> goes to index  1 \n",
      "Padding <P> goes to index  2\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    'Vocabulary size:',len(dataset.vocabulary),\n",
    "    '\\nMax label length:', dataset.max_label_length,\n",
    "    \"\\nStart <S> goes to index \",dataset.vocabulary['<S>'],\n",
    "      \"\\nEnd <E> goes to index \",dataset.vocabulary['<E>'],\n",
    "      \"\\nPadding <P> goes to index \",dataset.vocabulary['<P>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb2d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RATIO = 15\n",
    "\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac66c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gregory/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNetTransformer:\n\tMissing key(s) in state_dict: \"backbone.4.2.conv1.weight\", \"backbone.4.2.bn1.weight\", \"backbone.4.2.bn1.bias\", \"backbone.4.2.bn1.running_mean\", \"backbone.4.2.bn1.running_var\", \"backbone.4.2.conv2.weight\", \"backbone.4.2.bn2.weight\", \"backbone.4.2.bn2.bias\", \"backbone.4.2.bn2.running_mean\", \"backbone.4.2.bn2.running_var\", \"backbone.5.2.conv1.weight\", \"backbone.5.2.bn1.weight\", \"backbone.5.2.bn1.bias\", \"backbone.5.2.bn1.running_mean\", \"backbone.5.2.bn1.running_var\", \"backbone.5.2.conv2.weight\", \"backbone.5.2.bn2.weight\", \"backbone.5.2.bn2.bias\", \"backbone.5.2.bn2.running_mean\", \"backbone.5.2.bn2.running_var\", \"backbone.5.3.conv1.weight\", \"backbone.5.3.bn1.weight\", \"backbone.5.3.bn1.bias\", \"backbone.5.3.bn1.running_mean\", \"backbone.5.3.bn1.running_var\", \"backbone.5.3.conv2.weight\", \"backbone.5.3.bn2.weight\", \"backbone.5.3.bn2.bias\", \"backbone.5.3.bn2.running_mean\", \"backbone.5.3.bn2.running_var\", \"backbone.6.2.conv1.weight\", \"backbone.6.2.bn1.weight\", \"backbone.6.2.bn1.bias\", \"backbone.6.2.bn1.running_mean\", \"backbone.6.2.bn1.running_var\", \"backbone.6.2.conv2.weight\", \"backbone.6.2.bn2.weight\", \"backbone.6.2.bn2.bias\", \"backbone.6.2.bn2.running_mean\", \"backbone.6.2.bn2.running_var\", \"backbone.6.3.conv1.weight\", \"backbone.6.3.bn1.weight\", \"backbone.6.3.bn1.bias\", \"backbone.6.3.bn1.running_mean\", \"backbone.6.3.bn1.running_var\", \"backbone.6.3.conv2.weight\", \"backbone.6.3.bn2.weight\", \"backbone.6.3.bn2.bias\", \"backbone.6.3.bn2.running_mean\", \"backbone.6.3.bn2.running_var\", \"backbone.6.4.conv1.weight\", \"backbone.6.4.bn1.weight\", \"backbone.6.4.bn1.bias\", \"backbone.6.4.bn1.running_mean\", \"backbone.6.4.bn1.running_var\", \"backbone.6.4.conv2.weight\", \"backbone.6.4.bn2.weight\", \"backbone.6.4.bn2.bias\", \"backbone.6.4.bn2.running_mean\", \"backbone.6.4.bn2.running_var\", \"backbone.6.5.conv1.weight\", \"backbone.6.5.bn1.weight\", \"backbone.6.5.bn1.bias\", \"backbone.6.5.bn1.running_mean\", \"backbone.6.5.bn1.running_var\", \"backbone.6.5.conv2.weight\", \"backbone.6.5.bn2.weight\", \"backbone.6.5.bn2.bias\", \"backbone.6.5.bn2.running_mean\", \"backbone.6.5.bn2.running_var\". \n\tsize mismatch for image_positional_encoder.pe: copying a param with shape torch.Size([128, 1920, 1920]) from checkpoint, the shape in current model is torch.Size([128, 1024, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNetTransformer(dataset\u001b[38;5;241m=\u001b[39mdataset)\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModels_Parameters_Log/Collate2_final.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m lit_model \u001b[38;5;241m=\u001b[39m LitResNetTransformer(model\u001b[38;5;241m=\u001b[39mmodel, WandB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m lit_model\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNetTransformer:\n\tMissing key(s) in state_dict: \"backbone.4.2.conv1.weight\", \"backbone.4.2.bn1.weight\", \"backbone.4.2.bn1.bias\", \"backbone.4.2.bn1.running_mean\", \"backbone.4.2.bn1.running_var\", \"backbone.4.2.conv2.weight\", \"backbone.4.2.bn2.weight\", \"backbone.4.2.bn2.bias\", \"backbone.4.2.bn2.running_mean\", \"backbone.4.2.bn2.running_var\", \"backbone.5.2.conv1.weight\", \"backbone.5.2.bn1.weight\", \"backbone.5.2.bn1.bias\", \"backbone.5.2.bn1.running_mean\", \"backbone.5.2.bn1.running_var\", \"backbone.5.2.conv2.weight\", \"backbone.5.2.bn2.weight\", \"backbone.5.2.bn2.bias\", \"backbone.5.2.bn2.running_mean\", \"backbone.5.2.bn2.running_var\", \"backbone.5.3.conv1.weight\", \"backbone.5.3.bn1.weight\", \"backbone.5.3.bn1.bias\", \"backbone.5.3.bn1.running_mean\", \"backbone.5.3.bn1.running_var\", \"backbone.5.3.conv2.weight\", \"backbone.5.3.bn2.weight\", \"backbone.5.3.bn2.bias\", \"backbone.5.3.bn2.running_mean\", \"backbone.5.3.bn2.running_var\", \"backbone.6.2.conv1.weight\", \"backbone.6.2.bn1.weight\", \"backbone.6.2.bn1.bias\", \"backbone.6.2.bn1.running_mean\", \"backbone.6.2.bn1.running_var\", \"backbone.6.2.conv2.weight\", \"backbone.6.2.bn2.weight\", \"backbone.6.2.bn2.bias\", \"backbone.6.2.bn2.running_mean\", \"backbone.6.2.bn2.running_var\", \"backbone.6.3.conv1.weight\", \"backbone.6.3.bn1.weight\", \"backbone.6.3.bn1.bias\", \"backbone.6.3.bn1.running_mean\", \"backbone.6.3.bn1.running_var\", \"backbone.6.3.conv2.weight\", \"backbone.6.3.bn2.weight\", \"backbone.6.3.bn2.bias\", \"backbone.6.3.bn2.running_mean\", \"backbone.6.3.bn2.running_var\", \"backbone.6.4.conv1.weight\", \"backbone.6.4.bn1.weight\", \"backbone.6.4.bn1.bias\", \"backbone.6.4.bn1.running_mean\", \"backbone.6.4.bn1.running_var\", \"backbone.6.4.conv2.weight\", \"backbone.6.4.bn2.weight\", \"backbone.6.4.bn2.bias\", \"backbone.6.4.bn2.running_mean\", \"backbone.6.4.bn2.running_var\", \"backbone.6.5.conv1.weight\", \"backbone.6.5.bn1.weight\", \"backbone.6.5.bn1.bias\", \"backbone.6.5.bn1.running_mean\", \"backbone.6.5.bn1.running_var\", \"backbone.6.5.conv2.weight\", \"backbone.6.5.bn2.weight\", \"backbone.6.5.bn2.bias\", \"backbone.6.5.bn2.running_mean\", \"backbone.6.5.bn2.running_var\". \n\tsize mismatch for image_positional_encoder.pe: copying a param with shape torch.Size([128, 1920, 1920]) from checkpoint, the shape in current model is torch.Size([128, 1024, 1024])."
     ]
    }
   ],
   "source": [
    "model = ResNetTransformer(dataset=dataset).to(dev)\n",
    "model.load_state_dict(torch.load((\"Models_Parameters_Log/Collate2_final.pth\"), map_location=torch.device('cpu')))\n",
    "lit_model = LitResNetTransformer(model=model, WandB=False)\n",
    "lit_model.eval()\n",
    "lit_model.freeze()\n",
    "scripted_save = lit_model.to_torchscript()\n",
    "torch.jit.save(scripted_save, \"Models_Parameters_Log/scripted_model1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac25f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted = torch.jit.load(\"Models_Parameters_Log/scripted_model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/0a0df4c4229b725.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "image_tensor = padded_images\n",
    "print(display(transform(image_tensor)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f340dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 516 3\n",
      "58\n",
      "torch.Size([1, 58, 258])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAA6CAAAAAB+egj4AAAE+0lEQVR4nO2ZXYhVVRSAv3VnzjgzOteZNvgXZg/ZzJCVMglR+GJkYSIhIQaBmiQhFJVREUwUBU1RJCFEPdmDoEGIxhSYEoUY9KOUpJWCpUWabnNm7vzembN6OD937h1tZtqXuUF7PVzOWXf/rPPttdZe+xy5yP9dMpU2oPLiEXgEHgEeAR4BHgEeAR4BHgEeAR4BHgEeAR4BHgEeAR4BHgFQXYE5BbRIEaDDFbAjlgogMICNLmsChvJBFtBLU29ILC4I5JpRN339E+xlkNgJDEAtcG6uStNf4/Srakwvw/HaTkpcckEVksjb1JuJdZrBEymBDSIivCBzLsi4hgSNbBQRgfHbTk7E4TtC9Uz59G7Y+9Gf+0AnuDYGUbrzqTf013O8FUGT4Ii9g+58fG2TfnDixBoY3bQc4hIIIQooqwERndDazESAPATcjwJ1S462gspPzUkTA8Jzr2ajB5Ukcy4HpUURB4uvKC4+FVoLHMJagCUT6lOdLGKWvdEI26A/Dy1kkzaHBDoETOIQAHxGO6GFVxwsvqK4BAJghN1rLRgBtQ01AIRX42rBPLY9QmCQ6TlygyajajHHbtGCx6/shIWnFODnZrXJTHEOkTIHgntmmU+0WKswNdSKdJA59/BrjzBvFqx4/1oQ2QqnRT7HANuhK+7ZCzMMoQLczPnCkJ1YTraLAM2jKwiRByitKdzF3QvipUEBoWPVIvSe/cDqfajA029sbXiRpV+/txlz0dZOF6JlNVFQr/kQ+voxSOvxeLlXdaqNMwJkRhLnOHtdPKeSG3QyukTcETQ3fAPEBBTW7dboSlCGpqFJOSioNcizHRYgyMaJLY6LBA3m1zPLokiBP+amVZTh1u/jSf9zgXCmhdZnVFH2MBvYxfPRH6rwAUfoosjqFQBUhaD6EnB96YALlmGMMQBzSTNiD98lEbDZ2eYicfeC3+cBYDFVYaxVBB25DGbXg4o1QvxrDfLV0mSNAU7eGK3+KC/YsbFoikQd39YOpqoyifsZ4ZNNiUFhUaa6fJX2PQAB0pW1YBaObbDhy0cVMmfnS7UGv+UStZiLtnY6A0KBx0Cvq/WUA0GhVOm8TzQ3aHi5PVWNjG2/fzmQJS0DwpL/czPeBWAxAM2FPyzGDlQ13vQDYC7pW0+htb01DcBQj9MDOARCpgmEo4vjlGXSbSFKhVxu5PCdCmQ0TZZEDm8QhZ6GpDI2pIHQVJqe0rog2Xt2rD/SVtd/14E4slzTo9sx6XZgDW9GtwMcRk6TbNw7qSPcwuuAErtzxLsaWIBARCA+WiRBlDkoRVKYT+TeC90C62nTfg5EymNQcLp/JS5eEGRj+woZS4AvlkV5UQenCcCpGwDuOAygvLNFsRjRJ7dFCr0EmCNt52Yno8zZFAYKkg/Q3C97Ei+IjwfLD0J0dJBji8DdC1wDIZJ07y6SvvorzqhYzMcr49t8N5ANCg9SX1fSoXhHALqGozgSXberO4trVnTcFMcMJ+g/FrANNYJim6C7kdR4g5yfNYmlNLd9qwgq5dgey4xgfEnfFxTrJufNRlBafyxPnTjlb5CHeVwK52IADEOTfRhVOKFKXxksmnIvGL3/JZqdD2mUEyYqAeSpgaFyGDT1CEa/QS4oynvymZRUAMGY7wiZDGFpjTiFUolPKaVbRiWfH/9BDY8AjwCPAI8AjwCPAI8AjwCPAI8AjwD4G+uYrdB8t8sVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=258x58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scripted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#padded_images = torch.zeros(( 1, h, w))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#padded_images[ :,  :  h,  :  w] = image_tensor\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredicted formula:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m prediction \u001b[38;5;241m=\u001b[39m  \u001b[43mscripted\u001b[49m(image_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_to_strings(prediction))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(display(Math(token_to_strings(prediction))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scripted' is not defined"
     ]
    }
   ],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/zarhin.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = h//2\n",
    "w_new = w//2\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "#padded_images = torch.zeros(( 1, h, w))\n",
    "#padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "#print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/zarhin.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "image =  cv2.resize(image, (0, 0), fx=.5, fy=.5,  interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "#padded_images = torch.zeros(( 1, h, w))\n",
    "#padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "#print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = h//2\n",
    "w_new = w//2\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "#padded_images = torch.zeros(( 1, h, w))\n",
    "#padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "#print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/0a0df4c4229b725.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = 64\n",
    "w_new = int(h_new*ratio)\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(padded_images.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/screenshot_3.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = h//3\n",
    "w_new = w//3\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "image_tensor = Image_Transforms.train_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(padded_images.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32324406",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/0a0ebebfb1f6ab3.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "print(display(transform(image)))\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "ratio = int(w/h)\n",
    "h_new  = 64\n",
    "w_new = int(h_new*ratio)\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    \n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144414e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=  \"Jupyter_Notebooks/test_photos/Screen Shot_11.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = h//2\n",
    "w_new = w//2\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(padded_images.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_14.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = h//2\n",
    "w_new = w//2\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "#padded_images = torch.zeros(( 1, h, w))\n",
    "#padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "#print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_12.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "print(display(transform(image)))\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "\n",
    "\n",
    "    \n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "#ratio = int(w/h)\n",
    "#h_new  = 64\n",
    "#w_new = int(h_new*ratio)\n",
    "#print(h_new)\n",
    "#image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(padded_images.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13967559",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_10.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "ratio = int(w/h)\n",
    "h_new  = 100\n",
    "w_new = int(h_new*ratio)\n",
    "print(h_new)\n",
    "image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "padded_images = torch.zeros(( 1, h, w))\n",
    "padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(padded_images.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.asarray(image)\n",
    "\n",
    "\n",
    "h, w, c = image.shape\n",
    "print(h,w,c)\n",
    "\n",
    "#ratio = int(w/h)\n",
    "#h_new  = 100\n",
    "#w_new = h_new*ratio\n",
    "#print(h_new)\n",
    "#image = cv2.resize(image, (w_new, h_new), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) \n",
    "#c, h, w = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "#padded_images = torch.zeros(( 1, h, w))\n",
    "#padded_images[ :,  :  h,  :  w] = image_tensor\n",
    "\n",
    "#print(display(transform(padded_images)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prediction =  scripted_model(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print('\\nPredicted formula:')\n",
    "prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "print(token_to_strings(prediction))\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image.size\n",
    "print(image.size)\n",
    "w_new = w//2\n",
    "h_new = h//2\n",
    "image = image.resize((w_new, h_new), resample=Image.Resampling.BILINEAR)\n",
    "print(image.size)\n",
    "print(display(image)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29502cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 1774, 3)\n",
      "(101, 887, 3)\n",
      "torch.Size([1, 101, 887])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAABlCAAAAADLrkXHAAAX0UlEQVR4nO2deZhcRbXAf9V7Ty/Fi2xBCD5AIQExgBjgyWZQeASVRUhQFsWwBQK+D4hIWAQXNPlEQBZDAjw2jYD6vveAIItJFEgUZBEnKIsKBMIWyJ2Znq2np94f996+++1pZuwwM/X7Y6b7dlXduqfuuXXq1Km6YgMajabFJDZ2BTSacYjWO42m9Wi902haj9Y7jab1aL3TaFqP1juNpvVovdNoWo/WO42m9Wi902haj9Y7jab1aL3TaFqP1juNpvVovdNoWo/WO42m9Wi902haj9Y7jab1aL3TaFqP1juNpvVovdNoWo/WO42m9Wi902haT2pjV2BMUhZgbOxKaD7AaL0becoChNrYtdB8kNF25sgj1gixseug+WCj+7thIOufumru4zuv3qvlddGMKnR/NxyuFGIm9wpB0XNYTdtI9dGMFrTeDYfTNmODOAzu8xw1huhSkVJKbW+MT7TeDYNabj4PHKz+m4OQDkMX6dYCPRAcn4gNG7sGo5hkUXDWVQiUS39UVw0pUI06PQkCpecbxifazhkGReAq8+OG/90DAHV7gyxJgIEKcE4f7C0e+xfWT/OBRetdLBIqA3EJtoVHuQ3k8daByxmMLy+hUCmAbc7l7PUH+FOUhamVmjGNtjNjKCW4/xDVEfm7nLfwiT0QqO1f6lCebAJFtTuYgf85YsE8UAZI08ys25lZVD/FJD1vT9Km55hH610kEnMEFq0FUqBAAMqtd/KRWxdzxlkfC2aUCJRw6V1PNZtWYpCewVKiljSQ981g5X5a78Y82s6M5vf7DSFRfwbe2AJPWNi8Vf+ZvLa6KJBWIlD72N9eZiEDZYAEJSNBSgEvKDGUs2pGObq/i0Tu+qxq0N8BhgSfU9IOYwlklMYmKA6eeRKG3VlajlA1mEAoqmkQcWfUjBG03kXS2M5sklxWMHMpQGWAXFaghAKB2rAJWKHUhtR6Nx7QdmYkhjsAM4LoBCG6kwWW2j8IQGwHy7iUTVh5gHlIr2IYJ+h4leFxT/jhF0KP/tL7NalegkO5GKO2v1IKpf41apeUUsqGjxBNK9F2ZhyykZ0po+K8wsJVpHA7Pi1FEMffaphfhTIoC8RIx7BIeOYPx+fp0J3pBwfd3w0PBVykPPwuImkJuNKxJA3DMAzu4Fbr+yCA+JFATB/RGko6xdRT2wRlz+GE7gM3Jrq/i6Nhf5duCxuVbbUurL+TIV4aCQx2ghlAZtid4Aj2d8liNYPynzpRgt2/8G3twNlYaL/KsKjCoff5XP+i/Hq49fl48JCRQ/UB0GV+FwI1ot6VIpmQ8koc9DDnj+BpNM2h7cwYhvBQMrgX+He3Y1N1c3xo2tOBTt+x3t4+z3c1ODjC3pWnAeaBN270wIcv4j9G9DyaZhianSkJs30k9PWObHUip5wbZunpB6CQiqhp87ZbqmCt1PFu4uAjUTItze6q+3RCBbwYiZJgl2ebq8X7qjfulhFlAVMO/vHVcz0FSUC8PjG87HAZRuBNbFc4UHFR9h8Za0icgPbG7RZ4pJctG6mSyAOgOhBlxD6PhmQWKpuNLz6Td3+LD+0H4Os3AZdeTKqAdXrZKK8AUPl+MBe1hXUXu7/YIZv152XMOTZUOk7vBuHybyFUm08QCX+mEvCDpioQKfchZLVbpgywZg27en83nA7aXJlkMlAhWoahbeJPbAvaJ3AJzF7yvi5llCBJvLB9/due58ySsZoRsDPFOoBnKeRZC/xVQBlBaPMvEFXaYmuTZ0b98+7kYtMCcBM59djFUOBFgJcFvAg8EXOelZUX7Y+SV0XoArgnO/3+vCGx5+w5DbdKMTgfyLktzUiJz4j6IZRIuTfG0zJKKQ64zr0k3u3ITL7jPNFSxMgwrE0CiW1B+wUuhLjx/V3K6ECyhzLVriilZN6xffF3e3AIsxUAi2dz/RwAZfnhOk0nmGXOmZz32YxKx+8UOe8+l48hGZ3Opq27h73phgXfxDw9HwV4bkcnjXdX2MH9sJ8zSZj05ubBJHSWlBAq/gkUoJq2rqxBL21IJeh7Z9Osa6SmQlbh/bqpsxMj98Y4LbPQOnLhHPeaeHfazXAfd2QYJNAmgcS2oAMCV9N/e1RTVzC6yPCXJxU9Zsd+6iKOljmVjRuDBf0qSgFqNpxe4mSlyHAtis5BKHEhbrvRYCrx3UiKhSilvsLyZmIxDKMK8xQwQ5FDgVI7uW5+0bXclbrT2UaoiGBzjEASBjtRPO95yKSKxQbdb9WwaGQdV1kFm7mfb4ZhBC/3bGyv5ZCIlntj6i0jmGcd+hRc+uf29vb29val7Z7ESsH+SqnVbIFbhkECbRJIbAvaL/AeltNgJf6oJs/HgX4kCHEDBhsQsVGGAb0zIGGOmQ7vvOoGOow8Z06gNghw6vcO9KVVHNdgax5FN3dwAMCPmrgQowsF90LW2oXEWURaovSZ0DxpPmPNUgWSDCrYkYLrSCGZzL4PyzOMbvaCBnImyasI4kaKPmLk3hi7ZcrAQwDcDxd/fMqUKVOmTJk5xXwopKyki+E4akyj6pZhSKH+NqkGE9uC9gk8gRrKKGO0kuajKAwk7QKloMIx8WZSiD9Tmg666844dyEGyJOXKAwoJdj6NZ93ry0dP7GcKkClYE7aDsk3JwvddnllISDXg0DhcrYnSsH9gKzpbYlA9feEJkEifnVEMM/IIBHw1NQ4765EcPn5zfhpY+QeRaHewVTTltAFq6eZDeo+tyiD6MmZvseJb5iGpzjqbsOWobssJ6OvTUIT24KuC1yUAQSqty+szLGAtG/xxadYGw1IBCpmw46QKSolgEsuPWP2QgwoYbmhEmu3fg2+fmPBte9Btzz4N932+D3reaCpDmCgLt5K05LukErQi0DR10d9lVuJhx5iyl/+FjZaeJiT6IlOcqQquuy8WUubrFAMPXkl2C3eooemJqpj5R6BhIc3EfDUhnPslrkS9uJwv9qR4pLLyP/kTEDwBgDCHAlYMnSX5WTztklkYlvQR6piF/kMdx/9jR+zBX2hZY4JruUUDOAUlPmArCWJnf8Nm7+TCOA7F1ozMQLVVauHAD851d3+8q0t6iFRkhOPsY+vOnuzejvLRz+d726gdckctV48/Z35cAVFXy/IV7a1qjH3GgBUpWApt9PfWV1YPYn7pOk2bwcnj106ggvdJLs/Fbi5vSnunNlUBxsv91BShZUHWB+VId/aAtVRFhx1t+DquT7HTLpN7Dy5/TllWGJWvLSDGqg4ZoCnLCefp03CE9uCtv5n8ghyvfDQ9Ep4mWMAUxBlISyPlQFyl/bbjou+ynC92/H5+k0kV+1jSkmCmP9dvLtdSZydIuXSY12/OJKV+z5y7ZxYOZuWiIFX72yD12ziiW9Yt6FlQwLtO1v2il/v7CSek0rBmskG2JOKZ16rAGpd1rmaw389EgH/d1jIXkZ2gkmvonzBKXHEyx1yWdcX8zeJOGSZmLp8+RGmuWdu/nLPjJD6Jov1y5D2AF11KFwydJXlv06rTSIS24I2/9froSCizFGP9YixJfnSdhjICy6XG2JGYKFHv3KJ83m1Pf6Wk/lyrLxmzQo//ghzYqstYdmhpwS2I0kDICzrbZ3YxaoGQG+1ZO4n5HJV12cprCTBe/z+yQCWb3AnK1fONA7T58XW0cOj537ef6irqASfV+moLAleBYaudg3l7lY7uxUFy+ApvLf1jLA2q7mOrYNZP0dsbc0iEFOWt02iE1uC5v7Jsv6URInwMkc/trP5ClD86qjtbzkB2O/yuIsM0TvJFZcUKtjTL49YMwUp/soUQOQyg32ZZDBErCOXcb4M1G+xkvVf5FODvekMfb3FpM9JcNLNcK6/Gsk2t6e0mlZY3sD3mECfRGzzCuJ34ZsAvccE+iDVJqoDeeorz563fk6bribTZLIkMHB5aEnhPFb1H6nBzV9j87eicpSAM5s4Q1Du/otxu5FV/d/OeG7qw4dwosvgHGCt92iwLPxtEp34ed9/ADpkaJljAAHQD7fwDTjy6rNOPCFZI9znbhPUO8mKc1bs74h331+ZllgBYAAJJPJAIETM45VPpexfEwDdSCBZALJZoNjpnle+6SaE9406kCgiUIfda+m/y347iZuBVbwC7K9SYe7ak7jZsl7TaaBs1WVP6+c7j53Ovlevu+Tlt5w3iqSa6u/Cjn4aItUOgGY02y/34MU8qCZZKe09qiuJPGu8c4TFaasb3uY/hU8G1jKFlBVok+jEezr/D+MyQHBJRJljhkwP69gEQ849C0Sjp0tgfCfpze/yLJu/bY3w5Lqt7HGGQKnBZFuP6fK0LXtnfPeYK8B923/WR2kvfAxlIDNVJr4uQHUXQLl3g01nk3aQrjO+q48L6Cq4LyFREiiUOYTd4SVV35rLGd9ZSSpFYTlnzfoLrEBgcyy36DTzVjMDP4c7vsuZc1qRUaRy2aHNzVv45K4SnovxV7ju2hI3f9X+3TLwGkS6pAoCFJ0Jagr39IqrrHqdfG0SntgWtBS8PhHBBmu83aHCyhwLFFLm+O7gB475hTmgrQzIFQfGeY/8/Z2EPM/CWwLenZDtg4lYoa/n8AR/24luREGo83+4am8Ad6Pu4vr8OefjfL5jphRvCrVw3txrlADhDmesBu76RMm6bSa8S1FJwz2PAKybCNBV3OulsGuykhRRwtonFnMwMtE6H4BcgVt7hnkziCyC96hFz5XOBwLWaRx+ubsvhqgK/4SvIs3f+iMTeTCntiuDwbg2q6y65ANtEn5iW9B1gUvLECsbTrJ6oWOCHmssdekDd/7CnFWrwSIuicnijVeRsr513dnwIXLSfGaWAa5gDzEZgzIVxbXsDcDZvGxnrpVdmx3c4JR4FxciMRAMKu7hGrOb2X/RkiVLlixZclbIatCkLFHiKTBYD9yKzPGuqAeE7MlWa82L8wWcrOEinCSDAGfa9lPREsfQyUjpi84qlUrhSSkjOHyTaBNK8BQeY7khfrmD62IiOQtetFTT1TLRJCXAjhTqPhJHhnZZCXEP4W0SktgRtEvgArb8jfAk202c2rh2o4VBPs7twF5AmvO4H5VkKd+OyeK1M+UZ1wEKQ644EOATTxvI2TfaxhwoKxACgVIdZPINN/7Z6w/A1KeMUsLxJ/teXGUgvXZmW1oABz1oJEq7PQ3QlxZ1y3b5Z+ArtyNQnaWZd3rsTLOHt5PUko9+er+V/OBbD003LeLZi101lVu/1qDm8PpWZoJ6RMADnwvPItnziVhBlIVg1s+besD75I77YiLzCOCFHQyG0DKAGXAEoGxr1JahU5YUoAxC2qQjmBhH0OZ/+dPTMQ3kDdJwklmFjhVMQVB4Zio/XrPYnlOIk78vPnP1lqeenmwHbsmfeNn5u00CWMKRZIAvM0/RY3vKt6ID8jCd+LfX/GHLmSds9zQkYFPr0N+5G5yu0Tzo8ZP1cvi5n10D8PRB535vNhlQy1kNwIF1R0KJ+z2nqrCraTeZSbo4gpXwLaYDOfZlsSf1As83KX3LZCRvC2vfPZnjQ0KIWXBweBCm5MknUMRMbAuclQFDxSt398VE8x2KaodBhtQyAPRy6PyLjnC+12XolKW2sVIG2iQkcV3Q9v/TvsFHFSSV9CT7JSubkcVooDDAJ1TpvxZfoDBgHQtib4gN7m/WTTXYWb+7DHLZ0xYp2yTvqsnHPzXvh8y+UUFHMfFvGxo8VO2CDPnrIx+azu/3u+04Jryn/ryr4U0l/rmt09/Z/YuRsO26nry9gbk5X2zIu44xO87uqmf+dqfnBipWkra008MaEnHCLW4Xg8QzxSe/fWn980vbGeQzvfm/7IwBZHMInqvuSnB7IJN0mz+INLDgWP51crPhoF65Z90XE5mnPnFZatwygKsnr/uXLRnWy0oVWPBNZRDSJv2BxGb+E24xf7L+2xiuZGNrfAfy7c3PW1AZsNYQG6F7WHkYwj4P3jLkpusVCP75EaUE4uVJ3UPzFki3mfm9+W4jI91GLfXKNhiW3oUtDJfT/vjhtYPO7iQSketBrJ5mmPufW/FKf5ym6oEdZr1rqf1X2meOs9D40yfhJy/w2i+548sGEmEFyiERvLGFGaeBmL04NFblri959eyK5Nme78fchao150H3yN19MY3v12KyiZbx4pVhA4PVn9ipdMMbb2zhu9x02/pN10+I215hCPsadaDci1zWm/8+ohCIOZOacdLV3RrzvZrVL1JMMg3NbuZSDtvYcdeX17o3Beqiq1eIKdNAynroxgCfmi8cF+132Rs+x8o9GqsdFfZQcOZVV92tuN+Uo6V2JZKwhX3Wm4KbFUgEhS/5Sj/nGm+iu2h64sord/fFNCTZbMs4+GQIFFWkPyqQeJyqHYZXR9rYlAmxC4GGoHeqgrK8lwBK0YlSigHEVdcOXbZK0WX/UV7XXsY1zuMaEbpx6+JJnlPVKCil2jFACJdz/btznYHihdXH4GGlnoBJqHhvorVywjC64TYAHrTM8wSDmFExx1DjawjfGlTJidDllUPWDAGpkwOOjDt9GF65OxczFIk31TJevDJkgE4qkU8MX2JH0A0FPsboRnGH87XhU2dI+4ll8tSt8UyOvr5sjv6ewCAmllyG3v5slt7+fMYfO5kQDCYUg67hYOMCRVtCdTtZrLA1d52KCdVFMUFXTdJ4UyU7BEAClYIjOPlfV1qTxe65+TrFZHfBXvzhLst07rnKfnXrphXBLXfnYoaQsamWGV5ub2Jb0EMR+Ngil8URRGMJ6v2ibWy9SyaolZxVi+k263muoEMhlx570o0ukabMVb3exQKpwvVzPNo53syu8UjCvUNpPRg8JrXGQ61aNeVnqlIbwGS+aflVmMVN7tQFBKfi89gXAgswfkTD7ZE0o5tBd8hPw92EdH9n4wk1vftoJw5SMP0h086sdntSUV+O5l6akcjAZu94uzcpO8bSNLFm2Oj3I4SywvFNzYYzrCjFwOo6yUJQ+LfsEb5kaTo4EbwDNs14RutdKD32B8GNcETEXlh5mBfUsiBtwCIzgyg13iZFM/bR47tQjrV7JSvy2pwi9MfsZ+J0zm3h90MW0hJ4EKmfdRp9D4RykP/Ab+HDwYnvoxJHZ/zHTG7zjPkOAXrJs88q5j6uGneQmrGO1jsLs+N3ryozN15ZBM8B02EtClju3pHC98JyF7e6okXyLOdQ+uDN01ZxXWBrFs04RPszTSTM/3496NjZ9M/cRU+tOJB7ZtS6QKYHGk7ElYUzaWy6Z8zS8hnEadejXzSu0eM7m8PF98GOOKuy3tznx9pURhyImkEXyCHMwknB3yiYCie5eVO4zPyhhy9yPWGvTtCMN3R/Z+Ja+GR+deJ7bQ+JAcj+bKPuztxF0d53RABK8OL2BhST7jCG8RVGpfGi+zsTo6tS6eysVKwlDwaKVyjAM5aPpWaYa6mz3myJtnKxUC57nCsXoDzB6Qv6Md8UljSX/p4sBF8UY/k1HZqG6P4unFJiycmKgdQu7by5uRPu6guzDN2FzFxjuG5Le3jYvrM9Vrzp6+qHf1+E7ICmV+NpxhRa7yKQHH/7ui3tZQjWMf9WfVLAGe/9DEXVimVZuzXiS3eBOPRea7MR+Mg/zM307J1SBM/tNPKvhteMJrSdGYHBbbeuBljo2l09q+jxDMuU4uafAZtmhMk2wBdQWBvi9qGU+gdd5gYTy8w3N/LhnTAMrXbjGT1/F4Uhj0cJBa4Z8F7fHIAhP0S3At5xjv2JiVTtAV+vR7sOUdBVXMNaHaM53tF6F4kB5F2hmiERzW28i6Iy4B7n7UAn4UEsVvadm9zHUzMG0XZmLD09PXE/pyEJA3CP2MxEIFkGsE90rvPG6isCNENG93fD4nGewQCecFuai28AHo7Mo6fNNdqfOSxktt9+06XD3Gsi9tnUaGy03g0HKVD9PkvUE6+i0YSix3fDQin8A0ADpZQi/u1XmnGOHt8Nh94MwWk4I5Olv5k3KmvGH9rO1Ghaj7YzNZrWo/VOo2k9Wu80mtaj9U6jaT1a7zSa1qP1TqNpPVrvNJrWo/VOo2k9IxyvIsx9zUcwNtEMOe4ZgbCrqLpZx2MrnW5rmOSDiBWvPQIbdo5cM2hg5Pu7mnC99nhEmCOEWD8iJUXWTQghjovP++yIX1dLuGDEqj1yzaAB/h9evKELrLeNXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=887x101>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "image_path= \"Jupyter_Notebooks/test_photos/Screen Shot_13.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "image = cv2.resize(image, (0,0), fx=0.5, fy=0.5,interpolation=cv2.INTER_LINEAR)\n",
    "print(image.shape)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7a731a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 57, 3)\n",
      "(69, 171, 3)\n",
      "torch.Size([1, 69, 171])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=171x69>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "image_path= image_path= \"Jupyter_Notebooks/test_photos/0a0ebebfb1f6ab3.png\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "image = cv2.resize(image, (0,0), fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n",
    "print(image.shape)\n",
    "\n",
    "image_tensor = Image_Transforms.test_transform_with_padding(image=np.array(image))['image'][:1]\n",
    "print(image_tensor.shape)\n",
    "print(display(transform(image_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd592595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a366cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet34(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone1 = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    ")\n",
    "\n",
    "backbone2 = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "    resnet.layer4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d6355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90795fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backbone1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e8933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
