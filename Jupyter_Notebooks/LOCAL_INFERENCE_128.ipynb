{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f537eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR/Jupyter_Notebooks\n",
      "Current path:/Users/gregory/PROJECT_ML/PROJECT_AMINE/image-to-tex-OCR\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-16:] == 'image-to-tex-OCR':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b91c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.Data_Module import Data_Module\n",
    "from Models.Printed_Tex_Transformer import ResNetTransformer\n",
    "from Lightning_Models.Printed_Tex_Lit_Model import LitResNetTransformer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, Math\n",
    "from Data.image_transforms import Image_Transforms\n",
    "import cv2\n",
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "transform = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe5e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data Module by uploading images and formulas\n",
    "# images need to be in the folder Data/Data_Bank/generated_png_images\n",
    "# formulas need to be in Data/Data_Bank/final_png_formulas.txt\n",
    "# image filenames need to be in Data/Data_Bank/corresponding_png_images.txt\n",
    "\n",
    "dataset = Data_Module(stage = 'fit',\n",
    "                 set_max_label_length = 128,\n",
    "                 number_png_images_to_use_in_dataset=250*1000,\n",
    "                 labels_transform='default',\n",
    "                image_transform_name ='alb',\n",
    "                 train_val_fraction = 0.999,\n",
    "                   vocabulary_path = 'Tokenizer/230k_ver2.json',\n",
    "                load_vocabulary = True,\n",
    "                      image_padding = True,\n",
    "                      max_width = 10*1000,\n",
    "\n",
    "\n",
    "                 batch_size = 128,\n",
    "                num_workers = 8,\n",
    "                data_on_gpu = True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c2dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 581 \n",
      "Max label length: 130 \n",
      "Start <S> goes to index  0 \n",
      "End <E> goes to index  1 \n",
      "Padding <P> goes to index  2\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    'Vocabulary size:',len(dataset.vocabulary),\n",
    "    '\\nMax label length:', dataset.max_label_length,\n",
    "    \"\\nStart <S> goes to index \",dataset.vocabulary['<S>'],\n",
    "      \"\\nEnd <E> goes to index \",dataset.vocabulary['<E>'],\n",
    "      \"\\nPadding <P> goes to index \",dataset.vocabulary['<P>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3593d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetTransformer(dataset=dataset).to(dev)\n",
    "model.load_state_dict(torch.load((\"Models_Parameters_Log/RATIO15_nonNorm_noaspect_128by1440_2.pth\"), map_location=torch.device('cpu')))\n",
    "lit_model = LitResNetTransformer(model=model, WandB=False)\n",
    "lit_model.eval()\n",
    "lit_model.freeze()\n",
    "scripted_save = lit_model.to_torchscript()\n",
    "torch.jit.save(scripted_save, \"Models_Parameters_Log/scripted_model1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted = torch.jit.load(\"Models_Parameters_Log/scripted_model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from albumentations.augmentations.geometric.resize import Resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_RATIO = 10\n",
    "GOAL_HEIGHT = 64\n",
    "\n",
    "def find_ratio(h, w, goal):\n",
    "    return goal/h\n",
    "    \n",
    "\n",
    "def predict_new(image_path):\n",
    "    MAX_RATIO = 12\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.asarray(image)\n",
    "    h, w, c = image.shape\n",
    "    ascpect = w/h\n",
    "    if ascpect == 0:\n",
    "        ascpect = 1 \n",
    "    if ascpect > MAX_RATIO:\n",
    "        ascpect = MAX_RATIO\n",
    "    \n",
    "    downscale = GOAL_HEIGHT/h\n",
    "    image = cv2.resize(image, (0,0), fx=downscale, fy=downscale,interpolation=cv2.INTER_LINEAR)\n",
    "   \n",
    "    print(image.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    image_tensor = Image_Transforms.test_transform_with_padding(image =np.array(image))['image'][:1]\n",
    "    print(display(transform(image_tensor)))\n",
    "    print(image_tensor.shape)\n",
    "    print('\\nPredicted formula:')\n",
    "    prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "    print(token_to_strings(prediction))\n",
    "    print(display(Math(token_to_strings(prediction))))\n",
    "\n",
    "def predict(image_path):\n",
    "   \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.asarray(image)\n",
    "    h, w, c = image.shape\n",
    "    ratio =w / h\n",
    "    if ratio == 0:\n",
    "        ratio = 1 \n",
    "    if ratio > MAX_RATIO:\n",
    "        ratio = MAX_RATIO\n",
    "        \n",
    "    new_h = GOAL_HEIGHT\n",
    "    new_w = int(new_h * ratio)\n",
    "    image = Resize(interpolation= cv2.INTER_LINEAR,height=new_h, width= new_w, always_apply=True)(image=image)['image']\n",
    "    print(image.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    image_tensor = Image_Transforms.test_transform_with_padding(image =np.array(image))['image'][:1]\n",
    "    print(display(transform(image_tensor)))\n",
    "    \n",
    "    print(image_tensor.shape)\n",
    "    print('\\nPredicted formula:')\n",
    "    prediction =  scripted(image_tensor.unsqueeze(0))\n",
    "    print(token_to_strings(prediction))\n",
    "    print(display(Math(token_to_strings(prediction))))\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9e286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4885c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd3016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4858c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04e7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
