{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baf9121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting path:/home/resisistancerow_909/image-to-tex-OCR/Jupyter_Notebooks\n",
      "Current path:/home/resisistancerow_909/image-to-tex-OCR\n"
     ]
    }
   ],
   "source": [
    "# Here we take care of paths.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "print('Starting path:' + os.getcwd())\n",
    "if os.getcwd()[-16:] == 'image-to-tex-OCR':\n",
    "    pass\n",
    "else:\n",
    "    PATH = Path().resolve().parents[0]\n",
    "    os.chdir(PATH)\n",
    "\n",
    "# make sure you are in Paragraph_to_Tex folder\n",
    "print('Current path:' + os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe6035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Data.Data_Module import Data_Module\n",
    "from Models.Printed_Tex_Transformer import ResNetTransformer\n",
    "from Lightning_Models.Printed_Tex_Lit_Model import LitResNetTransformer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display, Math\n",
    "import cv2\n",
    "\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2436059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val Data is ready for Model loading.\n"
     ]
    }
   ],
   "source": [
    "# Generate Data Module by uploading images and formulas\n",
    "# images need to be in the folder Data/Data_Bank/generated_png_images\n",
    "# formulas need to be in Data/Data_Bank/final_png_formulas.txt\n",
    "# image filenames need to be in Data/Data_Bank/corresponding_png_images.txt\n",
    "\n",
    "dataset = Data_Module(stage = 'fit',\n",
    "                 set_max_label_length = 128,\n",
    "                 number_png_images_to_use_in_dataset=250*1000,\n",
    "                 labels_transform='default',\n",
    "                image_transform_name ='alb',\n",
    "                 train_val_fraction = 0.999,\n",
    "                   vocabulary_path = 'Data/Data_Bank/230k.json',\n",
    "                load_vocabulary = True,\n",
    "                      image_padding = True,\n",
    "                       max_width = 700,\n",
    "\n",
    "\n",
    "                 batch_size = 64,\n",
    "                num_workers = 8,\n",
    "                data_on_gpu = True,\n",
    "                )\n",
    "\n",
    "\n",
    "# Generates DataFrame and vocabulary, tokezniers etc\n",
    "dataset.prepare_data()\n",
    "\n",
    "# prepeares the dataloaders with transformations and splits train/val\n",
    "dataset.setup(stage = 'fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd9c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 579 \n",
      "Train-Dataset size: 223035 \n",
      "Validation-Dataset size: 224 \n",
      "Max label length: 130 \n",
      "Start <S> goes to index  0 \n",
      "End <E> goes to index  1 \n",
      "Padding <P> goes to index  2\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    'Vocabulary size:',len(dataset.vocabulary),\n",
    "    \"\\nTrain-Dataset size:\", len(dataset.data_train),\n",
    "    \"\\nValidation-Dataset size:\", len(dataset.data_val),\n",
    "    '\\nMax label length:', dataset.max_label_length,\n",
    "    \"\\nStart <S> goes to index \",dataset.vocabulary['<S>'],\n",
    "      \"\\nEnd <E> goes to index \",dataset.vocabulary['<E>'],\n",
    "      \"\\nPadding <P> goes to index \",dataset.vocabulary['<P>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60187ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/resisistancerow_909/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/resisistancerow_909/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initiate the model\n",
    "model = ResNetTransformer(dataset=dataset).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f73061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model here or woth lighithng using checkpoint.\n",
    "\n",
    "#model.load_state_dict(torch.load((\"Models_Parameters_Log/Printed1_2D600_350.pth\"), map_location=torch.device(dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f0d16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-955eed1fc66ac685\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-955eed1fc66ac685\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir Models_Parameters_Log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1796eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgmarus\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/resisistancerow_909/image-to-tex-OCR/wandb/run-20230327_171617-12r12v92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gmarus/image-to-tex-OCR/runs/12r12v92\" target=\"_blank\">astral-surf-246</a></strong> to <a href=\"https://wandb.ai/gmarus/image-to-tex-OCR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at Models_Parameters_Log/2D128_1920_NORM_WHITE/version_0/checkpoints/epoch=2-step=10455.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val Data is ready for Model loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/resisistancerow_909/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:346: UserWarning: The dirpath has changed from 'Models_Parameters_Log/2D128_1920_NORM_WHITE/version_0/checkpoints' to 'Models_Parameters_Log/2D128_1920_NORM_WHITE/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | ResNetTransformer  | 3.8 M \n",
      "1 | loss_fn  | CrossEntropyLoss   | 0     \n",
      "2 | val_cer  | CharacterErrorRate | 0     \n",
      "3 | test_cer | CharacterErrorRate | 0     \n",
      "------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.038    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at Models_Parameters_Log/2D128_1920_NORM_WHITE/version_0/checkpoints/epoch=2-step=10455.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0001f4619b8f41b29ea14b857007bc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 3485it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/resisistancerow_909/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\"Models_Parameters_Log\", name=\"2D128_1920_NORM_WHITE\")\n",
    "lit_model = LitResNetTransformer(model=model)\n",
    "\n",
    "#use the lit_model to run with Trainer\n",
    "trainer = Trainer(accelerator='gpu', devices=1,  max_epochs=60, logger=logger, auto_lr_find=True) #accumulate_grad_batches=2, overfit_batches=1, default_root_dir=\"lightning_logs/ResNet/checkpoints\"  \n",
    "trainer.fit(model=lit_model, datamodule=dataset,  ckpt_path='Models_Parameters_Log/2D128_1920_NORM_WHITE/version_0/checkpoints/epoch=2-step=10455.ckpt')\n",
    "\n",
    "#add parameter below to trainer.fit to continue from a checkpoint\n",
    "#ckpt_path='/Models_Parameters_Log/pritned1_inverted_epoch7.ckpt'\n",
    "# ckpt_path = '/Data/Data_Bank epoch=11-step=20916.ckpt'\n",
    "# ckpt_path='Models_Parameters_Log/2D128_1920_NORM_WHITE/version_0/checkpoints/epoch=2-step=10455.ckpt\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216e810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'epoch=37-step=53466.ckpt'\n",
    "import json\n",
    "\n",
    "def save_dic(dic):\n",
    "    filename = 'Models_Parameters_Log/258_Test_run_3.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(dic, default=str))\n",
    "\n",
    "def load_dic(filename):\n",
    "    with open(filename) as f:\n",
    "        dic = json.loads(f.read())\n",
    "    return dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f520236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'Models_Parameters_Log/Printed3_2D128_1920_NORM_WHITE.pth')\n",
    "\n",
    "# save vocabulary\n",
    "#save_dic(dataset.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faac9e",
   "metadata": {},
   "source": [
    "### TESTS\n",
    "In the cell below we pass input and target through the model and check inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8c1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import numpy as np\n",
    "transform = transforms.ToPILImage()\n",
    "# Helper Function to convert prediction labels to strings\n",
    "def token_to_strings(tokens):\n",
    "    mapping = dataset.vocabulary\n",
    "    inverse_mapping =dataset.inverse_vocabulary\n",
    "    s=''\n",
    "    if tokens.shape[0] ==1:\n",
    "        tokens = tokens[0]\n",
    "    for number in tokens:\n",
    "        letter = inverse_mapping[number.item()]\n",
    "        s= s +\" \" + str(letter)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897cfa92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c0cee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1920x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> { \\sf q } _ { 0 } { \\bf f } _ { 7 } { \\bf Z } _ { \\bf 7 } } { \\bf f } _ { \\bf 7 } } { \\bf f } _ { \\bf j } } <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> { \\sf q } _ { 0 } { \\bf f } _ { 7 } { \\bf Z } _ { \\bf 7 } } { \\bf f } _ { \\bf 7 } } { \\bf f } _ { \\bf j } } <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/my_image.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "\n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8566b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1920x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Predicted formula:\n",
      " <S> { \\bf \\Omega } { \\bf \\Omega } { \\bf Z } { \\bf f } { \\bf \\Omega } { \\bf Z } } { \\bf f } { \\bf \\Omega } { \\bf Z } } { \\bf I } } { \\bf { \\Omega } } } { \\bf { q } } } <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle  <S> { \\bf \\Omega } { \\bf \\Omega } { \\bf Z } { \\bf f } { \\bf \\Omega } { \\bf Z } } { \\bf f } { \\bf \\Omega } { \\bf Z } } { \\bf I } } { \\bf { \\Omega } } } { \\bf { q } } } <E> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/zarhin.png\"\n",
    "#my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image = cv2.imread(my_image_path)\n",
    "my_image =  cv2.cvtColor(my_image, cv2.COLOR_BGR2RGB)\n",
    "my_image= cv2.bitwise_not(my_image)\n",
    "#my_image = PIL.ImageOps.invert(my_image)\n",
    "\n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image'][:1]\n",
    "print(display(transform(my_image_tensor)))\n",
    "\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "870d74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/resisistancerow_909/anaconda3/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:1414: UserWarning: The image is already gray.\n",
      "  warnings.warn(\"The image is already gray.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (152,1920) (3,) (152,1920) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1288/3598318201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_image_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Jupyter_Notebooks/test_photos/screenshot_1.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmy_image_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transform_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPredicted formula:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_each_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                     )\n\u001b[1;32m    117\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, image, **params)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pixel_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_transform_init_args_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(img, mean, std, max_pixel_value)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnormalize_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalize_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mnormalize_numpy\u001b[0;34m(img, mean, denominator)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (152,1920) (3,) (152,1920) "
     ]
    }
   ],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_1.png\"\n",
    "my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image']\n",
    "print(display(my_image))\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad93eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_3.png\"\n",
    "my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image']\n",
    "print(display(my_image))\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c897f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALB\n",
    "\n",
    "my_image_path= \"Jupyter_Notebooks/test_photos/screenshot_2.png\"\n",
    "my_image = Image.open(my_image_path).convert(\"L\") \n",
    "my_image_tensor = dataset.image_transform_test(image=np.array(my_image))['image']\n",
    "print(display(my_image))\n",
    "print('\\nPredicted formula:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    my_prediction =  model.predict(my_image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(my_prediction))\n",
    "print(display(Math(token_to_strings(my_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0170849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf49985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec83ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISREGARD this uses wrong transformations\n",
    "\n",
    "# Get image and label from train data -- change number for different ones\n",
    "image_tensor, label = dataset.data_train[12]\n",
    "transform = transforms.ToPILImage()\n",
    "image = transform(image_tensor)\n",
    "\n",
    "print('\\nOriginal image and formula:')\n",
    "display(image)\n",
    "print(token_to_strings(label))\n",
    "print('\\nPredicted formula: \\n')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prediction =  model.predict(image_tensor.unsqueeze(0).to(dev))\n",
    "print(token_to_strings(prediction),'\\n')\n",
    "print(display(Math(token_to_strings(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c6102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
